{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Datasets for Geolocation Analysis and Feature engineering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered data saved to ../data/cleaned/fraud_data_engineered.csv\n",
      "        user_id    ip_address        country  hour_of_day  day_of_week  \\\n",
      "31545         2  8.802175e+08  United States           10            5   \n",
      "97542         4  2.785906e+09    Switzerland           21            5   \n",
      "12873         8  3.560567e+08  United States           11            3   \n",
      "104500       12  2.985180e+09         Mexico           20            2   \n",
      "21011        16  5.783125e+08  United States           12            3   \n",
      "\n",
      "        time_since_signup  tx_count_by_user  txn_velocity_hours  \n",
      "31545          990.273333                 1                -1.0  \n",
      "97542         2788.855278                 1                -1.0  \n",
      "12873         1852.000278                 1                -1.0  \n",
      "104500        1286.523611                 1                -1.0  \n",
      "21011          886.966667                 1                -1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FraudPreprocessor:\n",
    "    def __init__(self, fraud_path, ip_path):\n",
    "        self.fraud_path = fraud_path\n",
    "        self.ip_path = ip_path\n",
    "        self.fraud_df = None\n",
    "        self.ip_df = None\n",
    "        self.merged_df = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.fraud_df = pd.read_csv(self.fraud_path)\n",
    "        self.ip_df = pd.read_csv(self.ip_path)\n",
    "        return self\n",
    "\n",
    "    def preprocess_ips(self):\n",
    "        # Convert IPs in both datasets to int (they are already numeric, just ensure type)\n",
    "        self.fraud_df['ip_int'] = self.fraud_df['ip_address'].astype(np.int64)\n",
    "        self.ip_df['lower_bound_ip_int'] = self.ip_df['lower_bound_ip_address'].astype(np.int64)\n",
    "        self.ip_df['upper_bound_ip_int'] = self.ip_df['upper_bound_ip_address'].astype(np.int64)\n",
    "        return self\n",
    "\n",
    "    def merge_geolocation(self):\n",
    "        # Sort for asof merge\n",
    "        self.fraud_df = self.fraud_df.sort_values('ip_int')\n",
    "        self.ip_df = self.ip_df.sort_values('lower_bound_ip_int')\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            self.fraud_df,\n",
    "            self.ip_df,\n",
    "            left_on='ip_int',\n",
    "            right_on='lower_bound_ip_int',\n",
    "            direction='backward'\n",
    "        )\n",
    "\n",
    "        # Filter valid IP ranges\n",
    "        self.merged_df = merged[merged['ip_int'] <= merged['upper_bound_ip_int']].copy()\n",
    "        return self\n",
    "\n",
    "    def engineer_time_features(self):\n",
    "        df = self.merged_df\n",
    "\n",
    "        df['signup_time'] = pd.to_datetime(df['signup_time'])\n",
    "        df['purchase_time'] = pd.to_datetime(df['purchase_time'])\n",
    "        df['time_since_signup'] = (df['purchase_time'] - df['signup_time']).dt.total_seconds() / 3600.0  # in hours\n",
    "        df['hour_of_day'] = df['purchase_time'].dt.hour\n",
    "        df['day_of_week'] = df['purchase_time'].dt.dayofweek\n",
    "\n",
    "        self.merged_df = df\n",
    "        return self\n",
    "\n",
    "    def engineer_transaction_features(self):\n",
    "        df = self.merged_df.sort_values(['user_id', 'purchase_time'])\n",
    "        df['tx_count_by_user'] = df.groupby('user_id')['purchase_time'].transform('count')\n",
    "        df['prev_purchase_time'] = df.groupby('user_id')['purchase_time'].shift(1)\n",
    "        df['txn_velocity_hours'] = (\n",
    "            (df['purchase_time'] - df['prev_purchase_time']).dt.total_seconds() / 3600.0\n",
    "        )\n",
    "        df['txn_velocity_hours'] = df['txn_velocity_hours'].fillna(-1)\n",
    "        self.merged_df = df\n",
    "        return self\n",
    "\n",
    "    def save(self, save_path):\n",
    "        self.merged_df.to_csv(save_path, index=False)\n",
    "        print(f\"Engineered data saved to {save_path}\")\n",
    "\n",
    "    def run_all(self, save_path='../data/cleaned/fraud_data_engineered.csv'):\n",
    "        self.load_data()\\\n",
    "            .preprocess_ips()\\\n",
    "            .merge_geolocation()\\\n",
    "            .engineer_time_features()\\\n",
    "            .engineer_transaction_features()\n",
    "        self.save(save_path)\n",
    "        print(self.merged_df[[\n",
    "            'user_id', 'ip_address', 'country', 'hour_of_day', 'day_of_week',\n",
    "            'time_since_signup', 'tx_count_by_user', 'txn_velocity_hours'\n",
    "        ]].head())\n",
    "        return self.merged_df\n",
    "\n",
    "# Example usage:\n",
    "preprocessor = FraudPreprocessor(\n",
    "    fraud_path='../data/cleaned/fraud_data_cleaned.csv',\n",
    "    ip_path='../data/cleaned/ip_country_cleaned.csv'\n",
    ")\n",
    "engineered_df = preprocessor.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 129146 entries, 0 to 129145\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   user_id                 129146 non-null  int64  \n",
      " 1   signup_time             129146 non-null  object \n",
      " 2   purchase_time           129146 non-null  object \n",
      " 3   purchase_value          129146 non-null  int64  \n",
      " 4   device_id               129146 non-null  object \n",
      " 5   source                  129146 non-null  object \n",
      " 6   browser                 129146 non-null  object \n",
      " 7   sex                     129146 non-null  object \n",
      " 8   age                     129146 non-null  int64  \n",
      " 9   ip_address              129146 non-null  float64\n",
      " 10  class                   129146 non-null  int64  \n",
      " 11  ip_int                  129146 non-null  int64  \n",
      " 12  lower_bound_ip_address  129146 non-null  float64\n",
      " 13  upper_bound_ip_address  129146 non-null  float64\n",
      " 14  country                 129146 non-null  object \n",
      " 15  lower_bound_ip_int      129146 non-null  float64\n",
      " 16  upper_bound_ip_int      129146 non-null  float64\n",
      " 17  time_since_signup       129146 non-null  float64\n",
      " 18  hour_of_day             129146 non-null  int64  \n",
      " 19  day_of_week             129146 non-null  int64  \n",
      " 20  tx_count_by_user        129146 non-null  int64  \n",
      " 21  prev_purchase_time      0 non-null       float64\n",
      " 22  txn_velocity_hours      129146 non-null  float64\n",
      "dtypes: float64(8), int64(8), object(7)\n",
      "memory usage: 22.7+ MB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "   user_id          signup_time        purchase_time  purchase_value  \\\n",
      "0        2  2015-01-11 03:47:13  2015-02-21 10:03:37              54   \n",
      "1        4  2015-06-02 16:40:57  2015-09-26 21:32:16              41   \n",
      "2        8  2015-05-28 07:53:06  2015-08-13 11:53:07              47   \n",
      "3       12  2015-01-10 06:25:12  2015-03-04 20:56:37              35   \n",
      "4       16  2015-02-03 13:48:23  2015-03-12 12:46:23               9   \n",
      "\n",
      "       device_id  source browser sex  age    ip_address  ...  \\\n",
      "0  FGBQNDNBETFJJ     SEO  Chrome   F   25  8.802175e+08  ...   \n",
      "1  MKFUIVOHLJBYN  Direct  Safari   F   38  2.785906e+09  ...   \n",
      "2  SCQGQALXBUQZJ     SEO  Chrome   M   25  3.560567e+08  ...   \n",
      "3  MSNWCFEHKTIOY     Ads  Safari   M   19  2.985180e+09  ...   \n",
      "4  FROZWSSWOHZBE  Direct      IE   M   32  5.783125e+08  ...   \n",
      "\n",
      "   upper_bound_ip_address        country  lower_bound_ip_int  \\\n",
      "0            8.891924e+08  United States        8.724152e+08   \n",
      "1            2.786066e+09    Switzerland        2.785542e+09   \n",
      "2            3.690988e+08  United States        3.523215e+08   \n",
      "3            2.985296e+09         Mexico        2.985034e+09   \n",
      "4            5.872026e+08  United States        5.704253e+08   \n",
      "\n",
      "   upper_bound_ip_int time_since_signup  hour_of_day  day_of_week  \\\n",
      "0        8.891924e+08        990.273333           10            5   \n",
      "1        2.786066e+09       2788.855278           21            5   \n",
      "2        3.690988e+08       1852.000278           11            3   \n",
      "3        2.985296e+09       1286.523611           20            2   \n",
      "4        5.872026e+08        886.966667           12            3   \n",
      "\n",
      "   tx_count_by_user  prev_purchase_time  txn_velocity_hours  \n",
      "0                 1                 NaN                -1.0  \n",
      "1                 1                 NaN                -1.0  \n",
      "2                 1                 NaN                -1.0  \n",
      "3                 1                 NaN                -1.0  \n",
      "4                 1                 NaN                -1.0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Column dtypes:\n",
      "user_id                     int64\n",
      "signup_time                object\n",
      "purchase_time              object\n",
      "purchase_value              int64\n",
      "device_id                  object\n",
      "source                     object\n",
      "browser                    object\n",
      "sex                        object\n",
      "age                         int64\n",
      "ip_address                float64\n",
      "class                       int64\n",
      "ip_int                      int64\n",
      "lower_bound_ip_address    float64\n",
      "upper_bound_ip_address    float64\n",
      "country                    object\n",
      "lower_bound_ip_int        float64\n",
      "upper_bound_ip_int        float64\n",
      "time_since_signup         float64\n",
      "hour_of_day                 int64\n",
      "day_of_week                 int64\n",
      "tx_count_by_user            int64\n",
      "prev_purchase_time        float64\n",
      "txn_velocity_hours        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check your data structure\n",
    "engineered_df = pd.read_csv('../data/cleaned/fraud_data_engineered.csv')\n",
    "print(\"DataFrame info:\")\n",
    "print(engineered_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(engineered_df.head())\n",
    "print(\"\\nColumn dtypes:\")\n",
    "print(engineered_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "�� Class distribution BEFORE sampling:\n",
      "class\n",
      "0    0.90501\n",
      "1    0.09499\n",
      "Name: proportion, dtype: float64\n",
      "Categorical columns: ['country', 'browser', 'source', 'sex']\n",
      "Numerical columns: ['user_id', 'purchase_value', 'age', 'ip_address', 'lower_bound_ip_address', 'upper_bound_ip_address', 'time_since_signup', 'hour_of_day', 'day_of_week', 'tx_count_by_user', 'txn_velocity_hours']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinki/Desktop/KAIMProjects/week-8/fraud-detection-ecommerce-banking/venv/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Class distribution AFTER sampling:\n",
      "class\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "class FraudDataTransformer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_column='class',\n",
    "        scaling_method='standard',\n",
    "        sampling_method='smote',\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        categorical_cols=None,\n",
    "        exclude_cols=None,\n",
    "        max_categories=50  # Limit categories for high-cardinality columns\n",
    "    ):\n",
    "        self.target_column = target_column\n",
    "        self.scaling_method = scaling_method\n",
    "        self.sampling_method = sampling_method\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.exclude_cols = exclude_cols or []\n",
    "        self.max_categories = max_categories\n",
    "        self.numerical_cols = None\n",
    "        self.pipeline = None\n",
    "        self.preprocessor = None\n",
    "\n",
    "    def _get_scaler(self):\n",
    "        if self.scaling_method == 'standard':\n",
    "            return StandardScaler()\n",
    "        elif self.scaling_method == 'minmax':\n",
    "            return MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Choose 'standard' or 'minmax' for scaling_method.\")\n",
    "\n",
    "    def _get_sampler(self):\n",
    "        if self.sampling_method == 'smote':\n",
    "            return SMOTE(random_state=self.random_state)\n",
    "        elif self.sampling_method == 'undersample':\n",
    "            return RandomUnderSampler(random_state=self.random_state)\n",
    "        elif self.sampling_method == 'none':\n",
    "            return 'passthrough'\n",
    "        else:\n",
    "            raise ValueError(\"Choose 'smote', 'undersample', or 'none' for sampling_method.\")\n",
    "\n",
    "    def _identify_columns(self, X):\n",
    "        \"\"\"Identify categorical and numerical columns more robustly\"\"\"\n",
    "        # Always exclude these columns from numerical processing\n",
    "        always_exclude = ['signup_time', 'purchase_time', 'prev_purchase_time', 'ip_int', 'lower_bound_ip_int', 'upper_bound_ip_int']\n",
    "        \n",
    "        # If user specified categorical columns, use them\n",
    "        if self.categorical_cols is not None:\n",
    "            categorical = self.categorical_cols\n",
    "        else:\n",
    "            # Auto-detect categorical columns\n",
    "            categorical = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Numerical columns are all others except categorical and excluded\n",
    "        all_excluded = categorical + self.exclude_cols + always_exclude\n",
    "        numerical = [col for col in X.columns if col not in all_excluded]\n",
    "        \n",
    "        # Only include truly numerical columns\n",
    "        numerical = [col for col in numerical if X[col].dtype in ['int64', 'float64']]\n",
    "        \n",
    "        return categorical, numerical\n",
    "\n",
    "    def _build_preprocessor(self, X):\n",
    "        self.categorical_cols, self.numerical_cols = self._identify_columns(X)\n",
    "        \n",
    "        print(f\"Categorical columns: {self.categorical_cols}\")\n",
    "        print(f\"Numerical columns: {self.numerical_cols}\")\n",
    "        \n",
    "        transformers = []\n",
    "        \n",
    "        if self.numerical_cols:\n",
    "            transformers.append(('num', self._get_scaler(), self.numerical_cols))\n",
    "        \n",
    "        if self.categorical_cols:\n",
    "            # Use sparse=True to save memory\n",
    "            transformers.append(('cat', OneHotEncoder(\n",
    "                handle_unknown='ignore', \n",
    "                sparse=True,  # Use sparse matrix\n",
    "                max_categories=self.max_categories  # Limit categories\n",
    "            ), self.categorical_cols))\n",
    "\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=transformers,\n",
    "            sparse_threshold=0.3  # Use sparse output if more than 30% sparse\n",
    "        )\n",
    "        return self.preprocessor\n",
    "\n",
    "    def transform(self, df):\n",
    "        # 1. Split features and target\n",
    "        X = df.drop(columns=[self.target_column])\n",
    "        y = df[self.target_column]\n",
    "\n",
    "        # 2. Train-test split (stratified)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, stratify=y, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        print(\"\\n�� Class distribution BEFORE sampling:\")\n",
    "        print(y_train.value_counts(normalize=True).rename(\"proportion\"))\n",
    "\n",
    "        # 3. Build preprocessing pipeline\n",
    "        preprocessor = self._build_preprocessor(X_train)\n",
    "        sampler = self._get_sampler()\n",
    "\n",
    "        # 4. Define full transformation pipeline\n",
    "        self.pipeline = ImbPipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('sampler', sampler)\n",
    "        ])\n",
    "\n",
    "        # 5. Fit-transform on training set\n",
    "        X_train_resampled, y_train_resampled = self.pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "        # 6. Transform test set (only preprocessing, no sampling)\n",
    "        X_test_transformed = self.preprocessor.transform(X_test)\n",
    "\n",
    "        print(\"\\n✅ Class distribution AFTER sampling:\")\n",
    "        print(pd.Series(y_train_resampled).value_counts(normalize=True).rename(\"proportion\"))\n",
    "\n",
    "        return X_train_resampled, X_test_transformed, y_train_resampled, y_test\n",
    "\n",
    "# --- Example usage ---\n",
    "\n",
    "# Load your engineered data\n",
    "engineered_df = pd.read_csv('../data/cleaned/fraud_data_engineered.csv')\n",
    "\n",
    "# Specify your categorical columns (adjust as needed for your data)\n",
    "# Remove device_id if it has too many unique values\n",
    "categorical_cols = ['country', 'browser', 'source', 'sex']  # Removed device_id\n",
    "\n",
    "# If your target column is named 'class'\n",
    "transformer = FraudDataTransformer(\n",
    "    target_column='class',\n",
    "    scaling_method='standard',      # or 'minmax'\n",
    "    sampling_method='smote',        # or 'undersample' or 'none'\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    categorical_cols=categorical_cols,\n",
    "    max_categories=50  # Limit categories to prevent memory issues\n",
    ")\n",
    "\n",
    "X_train_res, X_test_trans, y_train_res, y_test = transformer.transform(engineered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
